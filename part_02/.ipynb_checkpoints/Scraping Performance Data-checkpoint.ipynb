{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Hockey Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow\n",
    "\n",
    "- I need to scrape several years of data for each team and all individual players\n",
    "- The team and player statistics will be in separate dataframes.\n",
    "- Aditionally, each year of data will also be in separate dataframes.\n",
    "- I will set up my scraper to grab team statistics for each team in a given year and make that a temporary dataframe which I will turn into individual csvs.\n",
    "- The individual player statistics will also be separated by year and saved into individual csvs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating base URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "base_url = 'https://www.hockey-reference.com/teams/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to grab a txt file of team links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    page = urlopen(base_url)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    file = open('hockey-reference_urls.txt', 'w')\n",
    "    file.write(str(soup))\n",
    "    file.close()\n",
    "\n",
    "def get_team_links(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_page(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hockey-reference_urls.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = open(\"hockey-reference_urls.txt\", 'r')\n",
    "soup = BeautifulSoup(page, \"lxml\")\n",
    "div = soup.find('div', {'class': 'overthrow table_container'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the team links in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_links = []\n",
    "links = div.find_all('a')\n",
    "for link in links:\n",
    "    team_links.append(link.get('href'))\n",
    "# As teams have moved and changed over the years\n",
    "# I had to manually add Arizona and Atlanta to this list\n",
    "team_links.insert(1, '/teams/ARI/')\n",
    "team_links.insert(2, '/teams/ATL/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/teams/ANA/',\n",
       " '/teams/ARI/',\n",
       " '/teams/ATL/',\n",
       " '/teams/PHX/',\n",
       " '/teams/BOS/',\n",
       " '/teams/BUF/',\n",
       " '/teams/CGY/',\n",
       " '/teams/CAR/',\n",
       " '/teams/CHI/',\n",
       " '/teams/COL/',\n",
       " '/teams/CBJ/',\n",
       " '/teams/DAL/',\n",
       " '/teams/DET/',\n",
       " '/teams/EDM/',\n",
       " '/teams/FLA/',\n",
       " '/teams/LAK/',\n",
       " '/teams/MIN/',\n",
       " '/teams/MTL/',\n",
       " '/teams/NSH/',\n",
       " '/teams/NJD/',\n",
       " '/teams/NYI/',\n",
       " '/teams/NYR/',\n",
       " '/teams/OTT/',\n",
       " '/teams/PHI/',\n",
       " '/teams/PIT/',\n",
       " '/teams/SJS/',\n",
       " '/teams/STL/',\n",
       " '/teams/TBL/',\n",
       " '/teams/TOR/',\n",
       " '/teams/VAN/',\n",
       " '/teams/VEG/',\n",
       " '/teams/WSH/',\n",
       " '/teams/WPG/']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for scraping individual player data\n",
    "\n",
    "- This function finds a specific table on each teams page which contains individual player stats for a given year.\n",
    "- There will be a for loop later which will call this function and iterate through each team and each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_table(url):\n",
    "    res = requests.get(url)\n",
    "    skater_soup = BeautifulSoup(res.content, 'lxml')\n",
    "    team_name = skater_soup.find('h1', {'itemprop': 'name'}).find_all('span')[1].text\n",
    "    table = skater_soup.find('div', {'id': 'all_skaters'}).find('table', {'id': 'skaters'}).find('tbody')\n",
    "    player_stats = []\n",
    "    for row in table.find_all('tr'):\n",
    "        players = {}\n",
    "        for element in row:\n",
    "            players['Player'] = row.find('a').text\n",
    "            players['Age'] = row.find('td', {'data-stat': 'age'}).text\n",
    "            players['Position'] = row.find('td', {'data-stat': 'pos'}).text\n",
    "            players['Games Played'] = row.find('td', {'data-stat': 'games_played'}).text\n",
    "            players['Goals'] = row.find('td', {'data-stat': 'goals'}).text\n",
    "            players['Assists'] = row.find('td', {'data-stat': 'assists'}).text\n",
    "            players['Points'] = row.find('td', {'data-stat': 'points'}).text\n",
    "            players['Plus Minus'] = row.find('td', {'data-stat': 'plus_minus'}).text\n",
    "            players['Penalty Minutes'] = row.find('td', {'data-stat': 'pen_min'}).text\n",
    "            players['ES Goals'] = row.find('td', {'data-stat': 'goals_ev'}).text\n",
    "            players['PP Goals'] = row.find('td', {'data-stat': 'goals_pp'}).text\n",
    "            players['SH Goals'] = row.find('td', {'data-stat': 'goals_sh'}).text\n",
    "            players['GW Goals'] = row.find('td', {'data-stat': 'goals_gw'}).text\n",
    "            players['ES Assists'] = row.find('td', {'data-stat': 'assists_ev'}).text\n",
    "            players['PP Assists'] = row.find('td', {'data-stat': 'assists_pp'}).text\n",
    "            players['SH Assists'] = row.find('td', {'data-stat': 'assists_sh'}).text\n",
    "            players['Shots'] = row.find('td', {'data-stat': 'shots'}).text\n",
    "            players['Shooting Perecentage'] = row.find('td', {'data-stat': 'shot_pct'}).text\n",
    "            players['Time on Ice'] = row.find('td', {'data-stat': 'time_on_ice'}).text\n",
    "            players['Time on Ice Avg'] = row.find('td', {'data-stat': 'time_on_ice_avg'}).text\n",
    "            players['Offenisve Point Shares'] = row.find('td', {'data-stat': 'ops'}).text\n",
    "            players['Defensive Point Shares'] = row.find('td', {'data-stat': 'dps'}).text\n",
    "            players['Point Shares'] = row.find('td', {'data-stat': 'ps'}).text\n",
    "            players['ES Blocks'] = row.find('td', {'data-stat': 'blocks'}).text\n",
    "            players['ES Hits'] = row.find('td', {'data-stat': 'hits'}).text\n",
    "            players['ES Face-Off Wins'] = row.find('td', {'data-stat': 'faceoff_wins'}).text\n",
    "            players['ES Face-Off Losses'] = row.find('td', {'data-stat': 'faceoff_losses'}).text\n",
    "            players['ES Face-Off Pct'] = row.find('td', {'data-stat': 'faceoff_percentage'}).text\n",
    "            players['Team'] = team_name\n",
    "        player_stats.append(players)\n",
    "    return player_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for scraping team statistics\n",
    "\n",
    "- This function finds a specific table on each teams page\n",
    "- I had to get creative in scraping this table as the data in this table was formatted differently than the individual player data table.\n",
    "- Once I find the specific table within the \"team_soup\" variable, this function uses the dictionary structure of the data to assign column names\n",
    "- Again this function will be called later in a for loop to iterate through team and year. Each year will get its own csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_table(url):\n",
    "    res = requests.get(url)\n",
    "    team_soup = BeautifulSoup(res.content, 'lxml')\n",
    "    team_name = team_soup.find('h1', {'itemprop': 'name'}).find_all('span')[1].text\n",
    "    table = team_soup.find('div', {'id': 'all_team_stats'}).find('table', {'id': 'team_stats'})\n",
    "    team_list = []\n",
    "    team = {'team': team_name}\n",
    "    for row in table.find('tbody').find('tr').find_all('td'):\n",
    "        stat = row.text\n",
    "        temp = row.attrs\n",
    "        column = temp['data-stat']\n",
    "        team.update({column: stat})\n",
    "    team_list.append(team)\n",
    "    return team_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for scraping year results data\n",
    "\n",
    "- This scrape will get me the finishing results for each team for each year.\n",
    "- Eventually this will be combined with the team statistics data for modeling purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.hockey-reference.com/leagues/NHL_2018.html'\n",
    "def get_team_season(url):\n",
    "    res = requests.get(url)\n",
    "    season_soup = BeautifulSoup(res.content, 'lxml')\n",
    "    table = season_soup.find_all('div', {'id': 'all_stats'})[2]#.find('div', {'class': 'table_outer_container'})\n",
    "    print(table)\n",
    "    #//*[@id=\"stats\"]/tbody\n",
    "    #stats > tbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-de0ba7fd2f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_team_season\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-146-f87c159021fa>\u001b[0m in \u001b[0;36mget_team_season\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mseason_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseason_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'all_stats'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#.find('div', {'class': 'table_outer_container'})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#//*[@id=\"all_stats\"]/div[3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "get_team_season(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These cells were used for testing on individual web pages\n",
    "- I saved the function calls as universal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_year = get_team_table('https://www.hockey-reference.com/teams/ANA/2018.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_year = get_player_table('https://www.hockey-reference.com/teams/ANA/2018.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For loop for scraping team statistic data\n",
    "\n",
    "- Because I need each year to be separate, I am manually calling each year in the loop which will be temporarily saved in a dataframe. Then that dataframe will be converted to a csv. Each csv will get its own dataframe name in a separate EDA notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.hockey-reference.com'\n",
    "teams = team_links\n",
    "years = ['2007.html']\n",
    "year_df = pd.DataFrame()\n",
    "for team in teams:\n",
    "    try:\n",
    "        for year in years:\n",
    "            url = base_url + team + year\n",
    "            team_year = get_team_table(url)\n",
    "            team_df = pd.DataFrame(team_year)\n",
    "            year_df = pd.concat([year_df, team_df], axis=0)\n",
    "            year_df.reset_index(drop=True, inplace=True)\n",
    "            cols=[i for i in year_df.columns if i not in ['team']]\n",
    "            for col in cols:\n",
    "                year_df[col]=pd.to_numeric(year_df[col])\n",
    "            time.sleep(3)               \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Phoenix Coyotes changed their name to the Arizona Coyotes in 2016.\n",
    "# My scraper was giving me dual entries for this team in a few years.\n",
    "# This cell was used to remove the duplicate information before saving to csv.\n",
    "\n",
    "year_df.drop([2], axis=0, inplace=True)\n",
    "year_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to CSV\n",
    "\n",
    "- Again each year was saved independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df.to_csv('2007 team stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For loop for scraping individual player stats\n",
    "\n",
    "- Similar to the previous for loop, this will output one year of data which will be saved to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.hockey-reference.com'\n",
    "teams = team_links\n",
    "years = ['2018.html']\n",
    "year_player_stats = pd.DataFrame()\n",
    "for team in teams:\n",
    "    try:\n",
    "        for year in years:\n",
    "            url = base_url + team + year\n",
    "            team_year = get_team_table(url)\n",
    "            team_df = pd.DataFrame(team_year)\n",
    "            year_player_stats = pd.concat([year_df, team_df], axis=0)\n",
    "            year_player_stats.reset_index(drop=True, inplace=True)\n",
    "            time.sleep(3)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_player_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to CSV\n",
    "\n",
    "- Again each year will be saved to csv. This csv will contain stats for every player in the league for that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df.to_csv('2011 skater stats.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
